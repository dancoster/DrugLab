{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%additional_python_modules backoff,aws-requests-auth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from typing import Union, List\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.functions import col, from_unixtime\n",
    "\n",
    "\n",
    "class LabelsLogLoader(object):\n",
    "\n",
    "    def __init__(self, spark: SparkSession, log_path: str):\n",
    "        self.__spark = spark\n",
    "        self.__log_path = log_path\n",
    "        self.__filter_source_types = None\n",
    "        self.__filter_source_ids = None\n",
    "        self.__filter_labels = None\n",
    "        self.__filter_applicants = None\n",
    "        self.__filter_duplicates = None\n",
    "        self.__filter_before_timestamp = None\n",
    "        self.__filter_after_timestamp = None\n",
    "        self.__drop_duplicates = None\n",
    "\n",
    "    def filter_by_source_type(self, source_types: Union[Union[str, None], List[Union[str, None]]]):\n",
    "        r\"\"\"\n",
    "        Filters by source types\n",
    "\n",
    "        :param source_types: Source type or list of source types to filter by. None is also supported.\n",
    "                             To search only when no source type was specified, pass a list with None inside.\n",
    "                             Passing just None (outside of list) will unset the filter\n",
    "        \"\"\"\n",
    "        if not isinstance(source_types, list) and source_types is not None:\n",
    "            source_types = [source_types]\n",
    "        self.__filter_source_types = source_types\n",
    "\n",
    "    def filter_by_source_id(self, source_ids: Union[Union[str, None], List[Union[str, None]]]):\n",
    "        r\"\"\"\n",
    "        Filters by source ids\n",
    "\n",
    "        :param source_ids: Source id or list of source ids to filter by. None is also supported.\n",
    "                             To search only when no source id was specified, pass a list with None inside.\n",
    "                             Passing just None (outside of list) will unset the filter\n",
    "        \"\"\"\n",
    "        if not isinstance(source_ids, list) and source_ids is not None:\n",
    "            source_ids = [source_ids]\n",
    "        self.__filter_source_ids = source_ids\n",
    "\n",
    "    def filter_by_label(self, labels: Union[Union[str, None], List[Union[str, None]]]):\n",
    "        r\"\"\"\n",
    "        Filters by labels\n",
    "\n",
    "        :param labels: Label or list of labels to filter by. None is also supported.\n",
    "                       To search only when no label was specified, pass a list with None inside.\n",
    "                       Passing just None (outside of list) will unset the filter\n",
    "        \"\"\"\n",
    "        if not isinstance(labels, list) and labels is not None:\n",
    "            labels = [labels]\n",
    "        self.__filter_labels = labels\n",
    "\n",
    "    def filter_by_applicant(self, ids: Union[str, List[str], None]):\n",
    "        r\"\"\"\n",
    "        Filters by applicant being evaluated (labeled)\n",
    "\n",
    "        :param ids: Id or list of ids to filter by.\n",
    "                    Passing just None (outside of list) will unset the filter\n",
    "        \"\"\"\n",
    "        if not isinstance(ids, list) and ids is not None:\n",
    "            ids = [ids]\n",
    "        self.__filter_applicants = ids\n",
    "\n",
    "    def filter_by_duplicate(self, ids: Union[str, List[str], None]):\n",
    "        r\"\"\"\n",
    "        Filters by duplicate being matched (labeled)\n",
    "\n",
    "        :param ids: Id or list of ids to filter by.\n",
    "                    Passing just None (outside of list) will unset the filter\n",
    "        \"\"\"\n",
    "        if not isinstance(ids, list) and ids is not None:\n",
    "            ids = [ids]\n",
    "        self.__filter_duplicates = ids\n",
    "\n",
    "    def __to_timestamp(self, timestamp: Union[None, int, datetime]) -> Union[None, int]:\n",
    "        r\"\"\"\n",
    "        Converts input to valid timestamp value for filters\n",
    "\n",
    "        :param timestamp: Timestamp input\n",
    "        :return: Timestamp output\n",
    "        \"\"\"\n",
    "        if isinstance(timestamp, datetime):\n",
    "            return round(timestamp.timestamp() * 1000)\n",
    "        return timestamp\n",
    "\n",
    "    def filter_after_timestamp(self, timestamp: Union[None, int, datetime]):\n",
    "        r\"\"\"\n",
    "        Filters logs that occurred after specific timestamp\n",
    "\n",
    "        :param timestamp: Timestamp or None to unset\n",
    "        \"\"\"\n",
    "        self.__filter_after_timestamp = self.__to_timestamp(timestamp)\n",
    "\n",
    "    def filter_before_timestamp(self, timestamp: Union[None, int, datetime]):\n",
    "        r\"\"\"\n",
    "        Filters logs that occurred before specific timestamp\n",
    "\n",
    "        :param timestamp: Timestamp or None to unset\n",
    "        \"\"\"\n",
    "        self.__filter_before_timestamp = self.__to_timestamp(timestamp)\n",
    "\n",
    "    def drop_duplicates(self, drop_duplicates: bool = True):\n",
    "        r\"\"\"\n",
    "        Should duplicates be dropped. Duplicate entries are those with same applicant and match id.\n",
    "        Only latest entry is kept.\n",
    "\n",
    "        :param drop_duplicates: If True (or non specified) will drop duplicates. If False will ignore this behaviour.\n",
    "        \"\"\"\n",
    "        self.__drop_duplicates = drop_duplicates\n",
    "\n",
    "    def load(self, date_field: bool = False, time_field: bool = False) -> DataFrame:\n",
    "        r\"\"\"\n",
    "        Loads logs and applies filters\n",
    "\n",
    "        Returned DataFrame has following schema\n",
    "        root\n",
    "         |-- timestamp: string (nullable = true)\n",
    "         |-- label: string (nullable = true)\n",
    "         |-- recordId: string (nullable = true)\n",
    "         |-- applicantId: string (nullable = true)\n",
    "         |-- employeeId: string (nullable = true)\n",
    "         |-- duplicateId: string (nullable = true)\n",
    "         |-- sourceType: string (nullable = true)\n",
    "         |-- sourceId: string (nullable = true)\n",
    "\n",
    "        If `date_field` is set to true then field representing date from timestamp in format `yyyy-MM-dd` is added\n",
    "          |-- date: string (nullable = true)\n",
    "\n",
    "        If `date_time` is set to true then field representing time from timestamp in format `HH:mm:ss` is added\n",
    "          |-- time: string (nullable = true)\n",
    "\n",
    "        :param date_field: Should field `date` be added\n",
    "        :param time_field: Should field `time` be added\n",
    "\n",
    "        :return: Dataframe of logs\n",
    "        \"\"\"\n",
    "\n",
    "        condition = None\n",
    "\n",
    "        # Update source type condition\n",
    "        if self.__filter_source_types is not None:\n",
    "            filter_source_type_condition = col('sourceType').isin([m for m in self.__filter_source_types if m is not None])\n",
    "            if None in self.__filter_source_types:\n",
    "                filter_source_type_condition = (filter_source_type_condition | col('sourceType').isNull())\n",
    "            condition = filter_source_type_condition if condition is None else condition & filter_source_type_condition\n",
    "\n",
    "        # Update source id condition\n",
    "        if self.__filter_source_ids is not None:\n",
    "            filter_source_id_condition = col('sourceId').isin([m for m in self.__filter_source_ids if m is not None])\n",
    "            if None in self.__filter_source_ids:\n",
    "                filter_source_id_condition = (filter_source_id_condition | col('sourceId').isNull())\n",
    "            condition = filter_source_id_condition if condition is None else condition & filter_source_id_condition\n",
    "\n",
    "        # Update label condition\n",
    "        if self.__filter_labels is not None:\n",
    "            filter_label_condition = col('label').isin([m for m in self.__filter_labels if m is not None])\n",
    "            if None in self.__filter_labels:\n",
    "                filter_label_condition = (filter_label_condition | col('label').isNull())\n",
    "            condition = filter_label_condition if condition is None else condition & filter_label_condition\n",
    "\n",
    "        # Update applicant condition\n",
    "        if self.__filter_applicants is not None:\n",
    "            filter_applicant_condition = col('applicantId1').isin([m for m in self.__filter_applicants if m is not None])\n",
    "            condition = filter_applicant_condition if condition is None else condition & filter_applicant_condition\n",
    "\n",
    "        # Update match condition\n",
    "        if self.__filter_duplicates is not None:\n",
    "            filter_match_condition = col('applicantId2').isin([m for m in self.__filter_duplicates if m is not None])\n",
    "            condition = filter_match_condition if condition is None else condition & filter_match_condition\n",
    "\n",
    "        # Filter before timestamp\n",
    "        if self.__filter_before_timestamp is not None:\n",
    "            filter_before_condition = col('timestamp') <= self.__filter_before_timestamp\n",
    "            condition = filter_before_condition if condition is None else condition & filter_before_condition\n",
    "\n",
    "        # Filter after timestamp\n",
    "        if self.__filter_after_timestamp is not None:\n",
    "            filter_after_condition = col('timestamp') >= self.__filter_after_timestamp\n",
    "            condition = filter_after_condition if condition is None else condition & filter_after_condition\n",
    "\n",
    "        labels = self.__spark.read.csv(self.__log_path, header=True).sort('timestamp', ascending=False)\n",
    "        results = labels if condition is None else labels.filter(condition)\n",
    "        \n",
    "        \n",
    "        temp = results.withColumnRenamed('applicantId1', 'applicantId').withColumnRenamed('applicantId2', 'duplicateId')\n",
    "        temp = temp.withColumnRenamed('applicantId', 'applicantId2').withColumnRenamed('duplicateId', 'applicantId1')\n",
    "        \n",
    "        results = results.union(temp)\n",
    "        \n",
    "        results = results.groupBy('applicantId1').agg(sf.collect_set('applicantId2').alias('duplicateIds'))\n",
    "        \n",
    "        results = results if not self.__drop_duplicates else results.dropDuplicates(['applicantId1'])\n",
    "        results = results.withColumnRenamed('applicantId1', 'applicantId').withColumnRenamed('applicantId2', 'duplicateId')\n",
    "\n",
    "        if date_field:\n",
    "            results = results.withColumn('date', from_unixtime(col('timestamp')/1000, 'yyyy-MM-dd'))\n",
    "\n",
    "        if time_field:\n",
    "            results = results.withColumn('time', from_unixtime(col('timestamp')/1000, 'HH:mm:ss'))\n",
    "\n",
    "        return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, List\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.functions import col, from_json, size, explode_outer as pyspark_explode, from_unixtime\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import StructType, StructField, ArrayType, StringType, DoubleType\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class QueryLogLoader:\n",
    "    r\"\"\"\n",
    "    Tool to help parse and filer detection call logs\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, spark: SparkSession, logs_path: str):\n",
    "        r\"\"\"\n",
    "\n",
    "        :param spark: Spark session\n",
    "        :param logs_path:  Path to logs\n",
    "        \"\"\"\n",
    "        self.__spark = spark\n",
    "        self.__logs_path = logs_path\n",
    "        self.__filter_applicants = None\n",
    "        self.__filter_max_results = None\n",
    "        self.__filter_metric_target = None\n",
    "        self.__filter_metric = None\n",
    "        self.__filter_models = None\n",
    "        self.__filter_has_matches = None\n",
    "        self.__filter_after_timestamp = None\n",
    "        self.__filter_before_timestamp = None\n",
    "\n",
    "    def filter_by_applicants(self, applicants: Union[Union[str, None], List[Union[str, None]]]):\n",
    "        r\"\"\"\n",
    "        Filters by requested applicants\n",
    "\n",
    "        :param applicants: Applicant or list of applicants to filter by. None is also supported.\n",
    "                           To search only when no model was specified, pass a list with None inside.\n",
    "                           Passing just None (outside of list) will unset the filter\n",
    "        \"\"\"\n",
    "        if not isinstance(applicants, list) and applicants is not None:\n",
    "            applicants = [applicants]\n",
    "        self.__filter_applicants = applicants\n",
    "\n",
    "    def filter_by_max_results(self, max_results: Union[int, List[int], None] = None):\n",
    "        r\"\"\"\n",
    "        Filters by requested max results\n",
    "\n",
    "        :param max_results: Max results or list of max results to filter by.\n",
    "                            Passing just None will unset the filter.\n",
    "        \"\"\"\n",
    "        if not isinstance(max_results, list) and max_results is not None:\n",
    "            max_results = [max_results]\n",
    "        self.__filter_max_results = max_results\n",
    "\n",
    "    def filter_by_metric_target(self, metric_target: Union[float, List[float], None] = None):\n",
    "        r\"\"\"\n",
    "        Filters by requested metric targets\n",
    "\n",
    "        :param metric_target: Metric target or list of metric targets to filter by.\n",
    "                              Passing just None will unset the filter.\n",
    "        \"\"\"\n",
    "        if not isinstance(metric_target, list) and metric_target is not None:\n",
    "            metric_target = [metric_target]\n",
    "        self.__filter_metric_target = metric_target\n",
    "\n",
    "    def filter_by_metric(self, metric: Union[str, List[str], None] = None):\n",
    "        r\"\"\"\n",
    "        Filters by requested metrics\n",
    "\n",
    "        :param metric: Metric or list of metrics to filter by.\n",
    "                       Passing just None will unset the filter.\n",
    "        \"\"\"\n",
    "        if not isinstance(metric, list) and metric is not None:\n",
    "            metric = [metric]\n",
    "        self.__filter_metric = metric\n",
    "\n",
    "    def filter_by_models(self, models: Union[Union[str, None], List[Union[str, None]]]):\n",
    "        r\"\"\"\n",
    "        Filters by requested models\n",
    "\n",
    "        :param models: Model or list of models to filter by. None is also supported.\n",
    "                       To search only when no model was specified, pass a list with None inside.\n",
    "                       Passing just None (outside of list) will unset the filter\n",
    "        \"\"\"\n",
    "        if not isinstance(models, list) and models is not None:\n",
    "            models = [models]\n",
    "        self.__filter_models = models\n",
    "\n",
    "    def filter_by_has_matches(self, has_matches: Union[None, bool]):\n",
    "        r\"\"\"\n",
    "        Filter based on did response have matches\n",
    "\n",
    "        :param has_matches: True, False to set filter, None to unset filter\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.__filter_has_matches = has_matches\n",
    "\n",
    "    def __to_timestamp(self, timestamp: Union[None, int, datetime]) -> Union[None, int]:\n",
    "        r\"\"\"\n",
    "        Converts input to valid timestamp value for filters\n",
    "\n",
    "        :param timestamp: Timestamp input\n",
    "        :return: Timestamp output\n",
    "        \"\"\"\n",
    "        if isinstance(timestamp, datetime):\n",
    "            return round(timestamp.timestamp() * 1000)\n",
    "        return timestamp\n",
    "\n",
    "    def filter_after_timestamp(self, timestamp: Union[None, int, datetime]):\n",
    "        r\"\"\"\n",
    "        Filters logs that occurred after specific timestamp\n",
    "\n",
    "        :param timestamp: Timestamp or None to unset\n",
    "        \"\"\"\n",
    "        self.__filter_after_timestamp = self.__to_timestamp(timestamp)\n",
    "\n",
    "    def filter_before_timestamp(self, timestamp: Union[None, int, datetime]):\n",
    "        r\"\"\"\n",
    "        Filters logs that occurred before specific timestamp\n",
    "\n",
    "        :param timestamp: Timestamp or None to unset\n",
    "        \"\"\"\n",
    "        self.__filter_before_timestamp = self.__to_timestamp(timestamp)\n",
    "\n",
    "    def load(self, explode: bool = False, date_field: bool = False, time_field: bool = False) -> DataFrame:\n",
    "        r\"\"\"\n",
    "        Loads logs and applies filters\n",
    "\n",
    "        Returned DataFrame has following schema\n",
    "        root\n",
    "         |-- model: string (nullable = true)\n",
    "         |-- smRequest: struct (nullable = true)\n",
    "         |    |-- applicantId: string (nullable = true)\n",
    "         |    |-- maxResults: long (nullable = true)\n",
    "         |    |-- metric: string (nullable = true)\n",
    "         |    |-- metricTarget: double (nullable = true)\n",
    "         |-- smResponse: struct (nullable = true)\n",
    "         |    |-- matches: array (nullable = true)\n",
    "         |    |    |-- element: struct (containsNull = true)\n",
    "         |    |    |    |-- duplicateId: string (nullable = true)\n",
    "         |    |    |    |-- probability: double (nullable = true)\n",
    "         |-- timestamp: long (nullable = true)\n",
    "\n",
    "         If `explode` is set to True, then smRequest is separated into columns and each of the matches\n",
    "         produces new row also separated into columns. DataFrame schema is then as follows:\n",
    "         root\n",
    "          |-- model: string (nullable = true)\n",
    "          |-- applicantId: string (nullable = true)\n",
    "          |-- maxResults: long (nullable = true)\n",
    "          |-- metric: string (nullable = true)\n",
    "          |-- metricTarget: double (nullable = true)\n",
    "          |-- duplicateId: string (nullable = true)\n",
    "          |-- probability: double (nullable = true)\n",
    "          |-- timestamp: long (nullable = true)\n",
    "        For cases where no matches were found, explode will create a row where duplicateId and probability are null\n",
    "\n",
    "        If `date_field` is set to true then field representing date from timestamp in format `yyyy-MM-dd` is added\n",
    "          |-- date: string (nullable = true)\n",
    "\n",
    "        If `date_time` is set to true then field representing time from timestamp in format `HH:mm:ss` is added\n",
    "          |-- time: string (nullable = true)\n",
    "\n",
    "        :param explode:    Should rows be exploded (description above)\n",
    "        :param date_field: Should field `date` be added\n",
    "        :param time_field: Should field `time` be added\n",
    "\n",
    "        :return: Dataframe of logs\n",
    "        \"\"\"\n",
    "\n",
    "        condition = None\n",
    "\n",
    "        # Update applicant condition\n",
    "        if self.__filter_applicants is not None:\n",
    "            filter_applicant_condition = col('smRequest.applicantId')\\\n",
    "                .isin([m for m in self.__filter_applicants if m is not None])\n",
    "            if None in self.__filter_applicants:\n",
    "                filter_applicant_condition = (filter_applicant_condition | col('smRequest.applicantId').isNull())\n",
    "            condition = filter_applicant_condition if condition is None else condition & filter_applicant_condition\n",
    "\n",
    "        # Update max results condition\n",
    "        if self.__filter_max_results is not None:\n",
    "            filter_max_results_condition = col('smRequest.maxResults') \\\n",
    "                .isin([m for m in self.__filter_max_results if m is not None])\n",
    "            condition = filter_max_results_condition if condition is None else condition & filter_max_results_condition\n",
    "\n",
    "        # Update precision condition\n",
    "        if self.__filter_metric_target is not None:\n",
    "            filter_metric_target_condition = col('smRequest.metricTarget') \\\n",
    "                .isin([m for m in self.__filter_metric_target if m is not None])\n",
    "            condition = filter_metric_target_condition if condition is None \\\n",
    "                else condition & filter_metric_target_condition\n",
    "\n",
    "        # Update max results condition\n",
    "        if self.__filter_metric is not None:\n",
    "            filter_metric_condition = col('smRequest.metric') \\\n",
    "                .isin([m for m in self.__filter_metric if m is not None])\n",
    "            condition = filter_metric_condition if condition is None else condition & filter_metric_condition\n",
    "\n",
    "        # Update model condition\n",
    "        if self.__filter_models is not None:\n",
    "            filter_model_condition = col('model').isin([m for m in self.__filter_models if m is not None])\n",
    "            if None in self.__filter_models:\n",
    "                filter_model_condition = (filter_model_condition | col('model').isNull())\n",
    "            condition = filter_model_condition if condition is None else condition & filter_model_condition\n",
    "\n",
    "        # Update has matches condition\n",
    "        if self.__filter_has_matches is not None:\n",
    "            filter_matches_condition = (size('smResponse.matches') > 0) if self.__filter_has_matches else (\n",
    "                    size('smResponse.matches') == 0)\n",
    "            condition = filter_matches_condition if condition is None else condition & filter_matches_condition\n",
    "\n",
    "        # Filter before timestamp\n",
    "        if self.__filter_before_timestamp is not None:\n",
    "            filter_before_condition = col('timestamp') <= self.__filter_before_timestamp\n",
    "            condition = filter_before_condition if condition is None else condition & filter_before_condition\n",
    "\n",
    "        # Filter after timestamp\n",
    "        if self.__filter_after_timestamp is not None:\n",
    "            filter_after_condition = col('timestamp') >= self.__filter_after_timestamp\n",
    "            condition = filter_after_condition if condition is None else condition & filter_after_condition\n",
    "\n",
    "        # Response is JSON stored as string. This needs to be parsed.\n",
    "        response_schema = StructType([\n",
    "            StructField(\n",
    "                name='matches',\n",
    "                nullable=False,\n",
    "                dataType=ArrayType(\n",
    "                    elementType=StructType(\n",
    "                        [\n",
    "                            StructField(name='duplicateId', dataType=StringType(), nullable=False),\n",
    "                            StructField(name='probability', dataType=DoubleType(), nullable=False)\n",
    "                        ]\n",
    "                    ),\n",
    "                    containsNull=False\n",
    "                )\n",
    "            )\n",
    "        ])\n",
    "\n",
    "        logs = self.__spark.read.json(path=self.__logs_path) \\\n",
    "            .withColumn('smResponse', from_json(col('smResponse'), response_schema))\n",
    "        results = (logs if condition is None else logs.filter(condition))\n",
    "\n",
    "        if explode:\n",
    "            results = results.select(\n",
    "                col('model'),\n",
    "                col('smRequest'),\n",
    "                pyspark_explode(col('smResponse.matches')).alias('match'),\n",
    "                col('timestamp')\n",
    "            )\n",
    "        results = results.select(\n",
    "                col('model'),\n",
    "                col('smRequest.applicantId').alias('applicantId'),\n",
    "                col('smRequest.maxResults').alias('maxResults'),\n",
    "                col('smRequest.metric').alias('metric'),\n",
    "                col('smRequest.metricTarget').alias('metricTarget'),\n",
    "                col('smResponse.matches').alias('match'),\n",
    "                col('timestamp')\n",
    "            )\n",
    "\n",
    "        if date_field:\n",
    "            results = results.withColumn('date', from_unixtime(col('timestamp')/1000, 'yyyy-MM-dd'))\n",
    "\n",
    "        if time_field:\n",
    "            results = results.withColumn('time', from_unixtime(col('timestamp')/1000, 'HH:mm:ss'))\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from typing import Union\n",
    "\n",
    "class LabeledRequestsSearcher(object):\n",
    "\n",
    "    def __init__(self, query_log_loader: QueryLogLoader, labels_log_loader: LabelsLogLoader):\n",
    "        self.__query_log_loader = query_log_loader\n",
    "        self.__labels_log_loader = labels_log_loader\n",
    "\n",
    "    def find_requests(self,\n",
    "                      since: datetime,\n",
    "                      until: datetime,\n",
    "                      max_results: int = 5,\n",
    "                      metric: str = \"PRECISION\",\n",
    "                      metric_target: float = 0.85,\n",
    "                      models: Union[str, None] = None,\n",
    "                      source_id: str = \"ALTEREGO_ON_HIRE\",\n",
    "                      source_type: str = \"VERIFIER\",\n",
    "                      labels: Union[str] = None,\n",
    "                      minutes_delta: int = 5):\n",
    "        # Setup Query log loader options\n",
    "        self.__query_log_loader.filter_by_metric(metric)\n",
    "        self.__query_log_loader.filter_by_metric_target(metric_target)\n",
    "        self.__query_log_loader.filter_by_applicants(None)\n",
    "        self.__query_log_loader.filter_by_max_results(max_results)\n",
    "        self.__query_log_loader.filter_by_has_matches(True)\n",
    "        self.__query_log_loader.filter_by_models([None] if models is None else models)\n",
    "        self.__query_log_loader.filter_before_timestamp(until)\n",
    "        self.__query_log_loader.filter_after_timestamp(since)\n",
    "\n",
    "        # Setup Label log loader options\n",
    "        self.__labels_log_loader.filter_by_applicant(None)\n",
    "        self.__labels_log_loader.filter_by_duplicate(None)\n",
    "        self.__labels_log_loader.filter_by_source_type(source_type)\n",
    "        self.__labels_log_loader.filter_by_source_id(source_id)\n",
    "        self.__labels_log_loader.filter_by_label(['DUPLICATE', 'NON_DUPLICATE'] if labels is None else labels)\n",
    "        self.__labels_log_loader.filter_before_timestamp(until)\n",
    "        self.__labels_log_loader.filter_after_timestamp(since)\n",
    "        self.__labels_log_loader.drop_duplicates()\n",
    "\n",
    "        query_logs = self.__query_log_loader.load(explode=False).cache()\n",
    "        label_logs = self.__labels_log_loader.load().cache()\n",
    "\n",
    "        joined = label_logs.join(\n",
    "            query_logs,\n",
    "            (query_logs.applicantId == label_logs.applicantId)\n",
    "#             & (query_logs.duplicateId == label_logs.duplicateId)\n",
    "            & (query_logs.timestamp < label_logs.timestamp)\n",
    "            & (label_logs.timestamp - query_logs.timestamp < 1000 * 60 * minutes_delta),\n",
    "            'inner')\n",
    "\n",
    "        return joined \\\n",
    "            .drop(label_logs.timestamp) \\\n",
    "            .drop(label_logs.recordId) \\\n",
    "#             .drop(label_logs.applicantId) \\\n",
    "            .drop(label_logs.employeeId) \\\n",
    "#             .drop(label_logs.duplicateId) \\\n",
    "            .drop(label_logs.sourceType) \\\n",
    "            .drop(label_logs.sourceId) \\\n",
    "            .drop(label_logs.label) \\\n",
    "            .drop(query_logs.model) \\\n",
    "            .drop(query_logs.probability) \\\n",
    "            .drop(query_logs.timestamp) \\\n",
    "            .drop(query_logs.duplicateId) \\\n",
    "            .dropDuplicates(['applicantId', 'maxResults', 'metric', 'metricTarget'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "import backoff\n",
    "import requests\n",
    "from aws_requests_auth.aws_auth import AWSRequestsAuth\n",
    "from botocore.client import BaseClient\n",
    "from requests.auth import AuthBase\n",
    "\n",
    "\n",
    "class AuthenticatedLambdaClient(object):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            sts_client: BaseClient,\n",
    "            sts_role: str,\n",
    "            sts_session: str,\n",
    "            hostname: str,\n",
    "            region: str,\n",
    "            stage: str,\n",
    "            token_life: int = 900,\n",
    "            token_buffer_time: int = 60):\n",
    "        self.__sts_client = sts_client\n",
    "        self.__sts_role = sts_role\n",
    "        self.__sts_session = sts_session\n",
    "        self.__hostname = hostname\n",
    "        self.__region = region\n",
    "        self.__stage = stage\n",
    "        self.__token_life = token_life\n",
    "        self.__token_buffer_time = token_buffer_time\n",
    "        self.__token = None\n",
    "\n",
    "    def __is_token_valid(self) -> bool:\n",
    "        r\"\"\"\n",
    "        Checks if token is still valid (not close to expiry or already expired)\n",
    "\n",
    "        :return: True if token is valid, False otherwise\n",
    "        \"\"\"\n",
    "        if self.__token is None:\n",
    "            return False\n",
    "        token_expires = self.__token['Credentials']['Expiration']\n",
    "        current_time = datetime.now(self.__token['Credentials']['Expiration'].tzinfo)\n",
    "        buffer_window = timedelta(seconds=self.__token_buffer_time)\n",
    "        return (token_expires - current_time) > buffer_window\n",
    "\n",
    "    def __get_token(self) -> dict:\n",
    "        r\"\"\"\n",
    "        Gets valid (non expired / close to expiry) token\n",
    "\n",
    "        :return: Valid token\n",
    "        \"\"\"\n",
    "        if not self.__is_token_valid():\n",
    "            self.__token = self.__sts_client.assume_role(\n",
    "                RoleArn=self.__sts_role,\n",
    "                RoleSessionName=self.__sts_session,\n",
    "                DurationSeconds=self.__token_life\n",
    "            )\n",
    "        return self.__token\n",
    "\n",
    "    def __get_auth(self) -> AWSRequestsAuth:\n",
    "        r\"\"\"\n",
    "        Creates authenticator for the call\n",
    "        \"\"\"\n",
    "        credentials = self.__get_token()\n",
    "        access_key = credentials['Credentials']['AccessKeyId']\n",
    "        secret_key = credentials['Credentials']['SecretAccessKey']\n",
    "        session_token = credentials['Credentials']['SessionToken']\n",
    "        return AWSRequestsAuth(aws_access_key=access_key,\n",
    "                               aws_secret_access_key=secret_key,\n",
    "                               aws_token=session_token,\n",
    "                               aws_host=self.__hostname,\n",
    "                               aws_region=self.__region,\n",
    "                               aws_service='execute-api')\n",
    "\n",
    "    def __create_uri(self, path: str) -> str:\n",
    "        path = path if not path.startswith('/') else path[1:]\n",
    "        return f'https://{self.__hostname}/{self.__stage}/{path}'\n",
    "\n",
    "    @backoff.on_exception(backoff.expo, requests.exceptions.ConnectionError, jitter=backoff.full_jitter, max_tries=5)\n",
    "    def request(self, method: str, path: str, auth: AuthBase = None, **kwargs) -> requests.Response:\n",
    "        r\"\"\"\n",
    "        Submits an API request to lambda\n",
    "        :param method: Request method\n",
    "        :param path:   API path (not including hostname and stage)\n",
    "        :param auth:   Optional authentication, STS token if not specified\n",
    "        :param kwargs: Other requests.request parameters\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return requests.request(\n",
    "            method=method,\n",
    "            url=self.__create_uri(path),\n",
    "            auth=auth if auth is not None else self.__get_auth(),\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    def post(self, path: str, auth: AuthBase = None, **kwargs) -> requests.Response:\n",
    "        r\"\"\"\n",
    "        Submits an POST request to lambda\n",
    "        :param path:   API path (not including hostname and stage)\n",
    "        :param auth:   Optional authentication, STS token if not specified\n",
    "        :param kwargs: Other requests.request parameters\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return self.request(method='POST', path=path, auth=auth, **kwargs)\n",
    "\n",
    "    def get(self, path: str, auth: AuthBase = None, **kwargs) -> requests.Response:\n",
    "        r\"\"\"\n",
    "        Submits an GET request to lambda\n",
    "        :param path:   API path (not including hostname and stage)\n",
    "        :param auth:   Optional authentication, STS token if not specified\n",
    "        :param kwargs: Other requests.request parameters\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return self.request(method='GET', path=path, auth=auth, **kwargs)\n",
    "\n",
    "    \n",
    "\n",
    "class AlterEgoClient(object):\n",
    "\n",
    "    def __init__(self, lambda_client: AuthenticatedLambdaClient):\n",
    "        self.__lambda_client = lambda_client\n",
    "\n",
    "    def query_duplicates(self, applicant_id: str, metric: str, metric_target: float, max_results: int, model: str = None, applicant_namespace: str = 'ICIMS', results_namespace: str = 'ICIMS') -> dict:\n",
    "        request = {\n",
    "            'applicantId': {\n",
    "                'namespace': applicant_namespace,\n",
    "                'id': applicant_id\n",
    "            },\n",
    "            'metric': metric,\n",
    "            'metricTarget': metric_target,\n",
    "            'resultsNamespace': results_namespace,\n",
    "            'maxResults': max_results\n",
    "        }\n",
    "        if model is not None:\n",
    "            request['model'] = model\n",
    "        print(request)\n",
    "\n",
    "        response = self.__lambda_client.post(path='/duplicate/query', json=request)\n",
    "        if response.status_code != 200:\n",
    "            return {\"error\":f\"{response.status_code} {response.text} error\"}\n",
    "        return response.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from typing import Union, Optional\n",
    "\n",
    "import dateutil\n",
    "from boto3 import client\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.types import Row\n",
    "\n",
    "class LabeledModelReplayJob(object):\n",
    "\n",
    "    def __init__(self, alter_ego_client: AlterEgoClient, labeled_requests_searcher: LabeledRequestsSearcher):\n",
    "        self.__alter_ego_client = alter_ego_client\n",
    "        self.__labeled_requests_searcher = labeled_requests_searcher\n",
    "\n",
    "    def __handle_row(self, row: Row, model: str, current: int, total: int):\n",
    "        result = self.__alter_ego_client.query_duplicates(\n",
    "            applicant_id=row.applicantId,\n",
    "            metric=row.metric,\n",
    "            metric_target=row.metricTarget,\n",
    "            max_results=row.maxResults,\n",
    "            model=model\n",
    "        )\n",
    "        data = {\n",
    "            'total': total,\n",
    "            'current': current,\n",
    "            'input': {\n",
    "                'applicantId': row.applicantId,\n",
    "                'metric': row.metric,\n",
    "                'metricTarget': row.metricTarget,\n",
    "                'maxResults': row.maxResults,\n",
    "                'model': model\n",
    "            },\n",
    "            'output': result\n",
    "        }\n",
    "        print(row.asDict())\n",
    "\n",
    "#         ground_truth = row.duplicateIds\n",
    "#         in_model_output = row.duplicateIds\n",
    "        print(f'Labeled Replay: {json.dumps(data)}')\n",
    "        \n",
    "    def get_replay_data(self, input_models: Union[str, None], since: datetime, until: datetime):\n",
    "        self.requests = self.__labeled_requests_searcher.find_requests(since=since, until=until, models=input_models).cache()\n",
    "        \n",
    "    def replay(self, input_models: Union[str, None], replay_model: str):\n",
    "        total = self.requests.count()\n",
    "        current = 0\n",
    "        for row in self.requests.collect():\n",
    "            current = current + 1\n",
    "            self.__handle_row(row=row, model=replay_model, current=current, total=total)\n",
    "            break\n",
    "\n",
    "\n",
    "def parse_time(in_time: str) -> Union[datetime, None]:\n",
    "    r\"\"\"\n",
    "    Parse time input\n",
    "    :param in_time:  Time input\n",
    "    :return: Parsed time input\n",
    "    \"\"\"\n",
    "    i = in_time.strip()\n",
    "    return None if len(i) == 0 else dateutil.parser.parse(i)\n",
    "\n",
    "\n",
    "def parse_model(model: str) -> Optional[str]:\n",
    "    r\"\"\"\n",
    "    Parse model input\n",
    "    :param model:      Name of model to filter for\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    in_model = model.strip()\n",
    "    return None if len(in_model) == 0 or in_model.lower() == 'none' else in_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read input parameters\n",
    "args = {\n",
    "    '--hostname': 'oxnjq1wfm8.execute-api.us-west-2.amazonaws.com',\n",
    "    '--stage': 'beta',\n",
    "    '--query_logs': 's3a://alterego-data-exports-prod/detections/monthly/20230129.gz',\n",
    "    '--label_logs': 's3a://alterego-data-exports-prod/labels/20230130T0509/',\n",
    "    '--in_model': 'COMPOUND_B',\n",
    "    '--out_model': 'COMPOUND_B_OS',\n",
    "    '--since': '2023-01-01 00:00:00',\n",
    "    '--until': '2023-01-30 00:00:00',\n",
    "    '--sts_role': 'arn:aws:iam::171837983301:role/CustomerRoleAlterEgoExperiments',\n",
    "    '--sts_session': 'LabeledModelReplay',\n",
    "}\n",
    "ARG_KEYS = ['sts_role', 'sts_session', 'hostname', 'stage', 'query_logs', 'label_logs', 'in_model', 'out_model',\n",
    "            'since', 'until']\n",
    "\n",
    "sts_role, sts_session, hostname, stage, query_logs, label_logs, in_model, out_model, since, until \\\n",
    "    = (args[f\"--{key}\"] for key in ARG_KEYS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup job\n",
    "lambda_client = AuthenticatedLambdaClient(client('sts'), sts_role, sts_session, hostname, 'us-west-2', stage)\n",
    "alter_ego_client = AlterEgoClient(lambda_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searcher = LabeledRequestsSearcher(QueryLogLoader(spark, query_logs), LabelsLogLoader(spark, label_logs))\n",
    "replay_job = LabeledModelReplayJob(alter_ego_client, searcher)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_job.replay(parse_model(in_model), parse_model(out_model), parse_time(since), parse_time(until))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drug_lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4 (main, Mar 31 2022, 03:37:37) [Clang 12.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bdfae57a338f302dc1dcec357ac11e9f753ccc771aa1c4e860bf04580c454473"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
